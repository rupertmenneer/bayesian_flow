{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/rupertmenneer/Documents/git/bayesian_flow/')\n",
    "from discretised.bfn_discretised_data import DiscretisedBimodalData\n",
    "from discretised.bfn_discretised import BayesianFlowNetworkDiscretised\n",
    "from bfn.models import SimpleNeuralNetworkDiscretised\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 4]) torch.Size([8, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# original \n",
    "def discretised_cdf(mu, sigma, x):\n",
    "    if x < -1:\n",
    "        return 0\n",
    "    if x > 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.5 * (1 + torch.erf((x-mu) / ( sigma*torch.sqrt( torch.tensor(2.0) ) ) ) )\n",
    "    \n",
    "\n",
    "def vectorised_cdf(mu, sigma, x):\n",
    "\n",
    "    # ensure shapes align for correct broadcasting\n",
    "    mu = mu.unsqueeze(-1)  # Shape: [B, D, 1]\n",
    "    sigma = sigma.unsqueeze(-1)  # Shape: [B, D, 1]\n",
    "    x = x.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, K]\n",
    "    assert mu.dim() == sigma.dim() == x.dim()\n",
    "\n",
    "    cdf_func = 0.5 * (1 + torch.erf((x - mu) / (sigma * torch.sqrt(torch.tensor(2.0)))))\n",
    "\n",
    "    # Apply conditions directly without squeezing, using broadcasting\n",
    "    lower_mask = x < -1\n",
    "    upper_mask = x > 1\n",
    "\n",
    "    # Apply masks\n",
    "    cdf_func = torch.where(lower_mask, torch.zeros_like(cdf_func), cdf_func)\n",
    "    cdf_func = torch.where(upper_mask, torch.ones_like(cdf_func), cdf_func)\n",
    "\n",
    "    return cdf_func\n",
    "\n",
    "\n",
    "batch = 8\n",
    "k = 4\n",
    "d = 1\n",
    "\n",
    "upper_bounds = torch.linspace(-1.1, 1.1, k)\n",
    "mu = torch.randn((batch, d))\n",
    "sigma = torch.randn((batch, d))\n",
    "\n",
    "# Correct the loop variable shadowing issue\n",
    "result_discretised = torch.zeros((batch, d, k))\n",
    "for i in range(batch):\n",
    "    for j in range(d):\n",
    "        for l in range(k):  # Use a different variable here to avoid shadowing\n",
    "            result_discretised[i, j, l] = discretised_cdf(mu[i, j], sigma[i, j], upper_bounds[l])\n",
    "\n",
    "result_vectorised = vectorised_cdf(mu, sigma, upper_bounds)\n",
    "\n",
    "print(result_vectorised.shape, result_discretised.shape)\n",
    "torch.allclose(result_discretised, result_vectorised)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# original \n",
    "def discretised_cdf(mu, sigma, x):\n",
    "    if x < -1:\n",
    "        return 0\n",
    "    if x > 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.5 * (1 + torch.erf((x-mu) / ( sigma*torch.sqrt( torch.tensor(2.0) ) ) ) )\n",
    "\n",
    "def vectorised_cdf_fixed(mu, sigma, x):\n",
    "    # ensure shapes align for correct broadcasting\n",
    "    mu = mu.unsqueeze(-1)  # Shape: [B, D, 1]\n",
    "    sigma = sigma.unsqueeze(-1)  # Shape: [B, D, 1]\n",
    "    x_expanded = x.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, K]\n",
    "\n",
    "    cdf_func = 0.5 * (1 + torch.erf((x_expanded - mu) / (sigma * torch.sqrt(torch.tensor(2.0)))))\n",
    "\n",
    "    # Apply conditions directly without squeezing, using broadcasting\n",
    "    lower_mask = x_expanded < -1\n",
    "    upper_mask = x_expanded > 1\n",
    "\n",
    "    # Apply masks\n",
    "    cdf_func = torch.where(lower_mask, torch.zeros_like(cdf_func), cdf_func)\n",
    "    cdf_func = torch.where(upper_mask, torch.ones_like(cdf_func), cdf_func)\n",
    "\n",
    "    return cdf_func\n",
    "\n",
    "batch = 8\n",
    "k = 4\n",
    "d = 1\n",
    "\n",
    "upper_bounds = torch.linspace(-1.1, 1.1, k)\n",
    "mu = torch.randn((batch, d))\n",
    "sigma = torch.randn((batch, d))\n",
    "\n",
    "# Correct the loop variable shadowing issue\n",
    "result_discretised_fixed = torch.zeros((batch, d, k))\n",
    "for i in range(batch):\n",
    "    for j in range(d):\n",
    "        for l in range(k):  # Use a different variable here to avoid shadowing\n",
    "            result_discretised_fixed[i, j, l] = discretised_cdf(mu[i, j], sigma[i, j], upper_bounds[l])\n",
    "\n",
    "result_vectorised_fixed = vectorised_cdf_fixed(mu, sigma, upper_bounds)\n",
    "\n",
    "print(torch.allclose(result_discretised_fixed, result_vectorised_fixed, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 2.725398302078247\n",
      "Epoch 2/1000, Loss: 2.975722074508667\n",
      "Epoch 3/1000, Loss: 2.170651912689209\n",
      "Epoch 4/1000, Loss: 1.492960810661316\n",
      "Epoch 5/1000, Loss: 1.0062901973724365\n",
      "Epoch 6/1000, Loss: 0.8573547005653381\n",
      "Epoch 7/1000, Loss: 1.0479390621185303\n",
      "Epoch 8/1000, Loss: 0.6261180639266968\n",
      "Epoch 9/1000, Loss: 0.6320828199386597\n",
      "Epoch 10/1000, Loss: 0.9604189395904541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     11\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mbfn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinuous_time_loss_for_discretised_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/git/bayesian_flow/discretised/bfn_discretised.py:131\u001b[0m, in \u001b[0;36mBayesianFlowNetworkDiscretised.continuous_time_loss_for_discretised_data\u001b[0;34m(self, discretised_data)\u001b[0m\n\u001b[1;32m    128\u001b[0m sender_mu_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_from_closed_form_bayesian_update(discretised_data, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Shape-> B*D*K bins pass the noisy samples to the model, output a continuous distribution and rediscretise it\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m output_distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscretised_output_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender_mu_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m k_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_bin_centers()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Shape-> B*D sum out over final distribution - weighted sums\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/git/bayesian_flow/discretised/bfn_discretised.py:104\u001b[0m, in \u001b[0;36mBayesianFlowNetworkDiscretised.discretised_output_distribution\u001b[0;34m(self, mu, t, gamma, t_min)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mu\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[0;32m--> 104\u001b[0m             discretised_output_dist[b, d, k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretised_cdf(mu_x[b, d], sigma_x[b, d], upper_bounds[k]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretised_cdf(mu_x[b, d], sigma_x[b, d], lower_bounds[k])\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m discretised_output_dist\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = DiscretisedBimodalData(k=16)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "bfn_model = BayesianFlowNetworkDiscretised(SimpleNeuralNetworkDiscretised())\n",
    "optim = AdamW(bfn_model.parameters(), lr=3e-3, betas=(0.9, 0.98), weight_decay=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    for _, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "        loss = bfn_model.continuous_time_loss_for_discretised_data(batch)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch {i+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is 1 DIMENSIONAL (D), with 16 BINS (K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DiscretisedBimodalData(k=16)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "print(batch.shape)\n",
    "# plt.hist(batch.numpy(), bins=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = bfn_model.sample_generation_for_discretised_data(n_steps=20, bs=1024).to(torch.float32)\n",
    "plt.hist(samples.detach().numpy(), bins=16)\n",
    "plt.hist(batch.numpy(), bins=16, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ls ../bfn_github/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bfn_github.bfn import BayesianFlowNetwork\n",
    "import torch\n",
    "# network should learn:\n",
    "# when x0 = 0, x1 = 1\n",
    "# when x0 = 1, x1 = 0\n",
    "def get_datapoint(batch=128, device='cpu'):\n",
    "    x0 = torch.randint(0, 2, size=(batch,), dtype=torch.bool, device=device)\n",
    "    x1 = ~x0\n",
    "\n",
    "    X = torch.stack([x0, x1], dim=0)\n",
    "    return X.long().transpose(0, 1)\n",
    "\n",
    "X = get_datapoint()  # (B, D=2) with K=2 classes \n",
    "print(X.shape)\n",
    "# plt.title(\"Dataset\")\n",
    "# plt.scatter(X[:, 0], X[:, 1]);\n",
    "# plt.grid()\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "bfn = BayesianFlowNetwork()\n",
    "\n",
    "optim = AdamW(bfn.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "n = 1000\n",
    "losses = []\n",
    "for i in tqdm(range(n)):\n",
    "    optim.zero_grad()\n",
    "    X = get_datapoint(device='cpu')\n",
    "    loss = bfn.process(X)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "bfn.sample(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.one_hot(torch.tensor([2, 1]).unsqueeze(0), num_classes=3)\n",
    "def vectorised_discretised_cdf(mu, sigma, bounds):\n",
    "    # input is mu, sigma -> B x D, and bounds -> K\n",
    "    lower_mask = bounds < -1\n",
    "    upper_mask = bounds > 1\n",
    "\n",
    "    # output is B x D x K\n",
    "    result = torch.zeros(mu.shape[0], mu.shape[1], bounds.shape[0])\n",
    "    result = 0.5 * (1 + torch.erf((bounds - mu.unsqueeze(-1)) / (sigma.unsqueeze(-1) * torch.sqrt(torch.tensor(2.0)))))\n",
    "    # clip result depending on boundsa\n",
    "    result[:, :, lower_mask] = 0\n",
    "    result[:, :, upper_mask] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "mu = torch.randn(2, 2)\n",
    "sigma = torch.ones(2, 2)\n",
    "bounds = torch.linspace(-1.1, 1.1, 4)\n",
    "\n",
    "cdf = vectorised_discretised_cdf(mu, sigma, bounds)\n",
    "\n",
    "print(cdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
