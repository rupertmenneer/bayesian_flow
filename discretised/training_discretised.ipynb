{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/rupertmenneer/Documents/git/bayesian_flow/')\n",
    "from discretised.bfn_discretised_data import DiscretisedBimodalData\n",
    "from discretised.bfn_discretised import BayesianFlowNetworkDiscretised\n",
    "from bfn.models import SimpleNeuralNetworkDiscretised\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.8820710182189941\n",
      "Epoch 2/1000, Loss: 1.1048903465270996\n",
      "Epoch 3/1000, Loss: 0.8282839059829712\n",
      "Epoch 4/1000, Loss: 0.9115344285964966\n",
      "Epoch 5/1000, Loss: 0.6085684895515442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m bfn_model\u001b[38;5;241m.\u001b[39mcontinuous_time_loss_for_discretised_data(batch)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Documents/ml/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ml/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = DiscretisedBimodalData(k=16)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "bfn_model = BayesianFlowNetworkDiscretised(SimpleNeuralNetworkDiscretised())\n",
    "optim = AdamW(bfn_model.parameters(), lr=3e-3, betas=(0.9, 0.98), weight_decay=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    for _, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "        loss = bfn_model.continuous_time_loss_for_discretised_data(batch)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch {i+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is 1 DIMENSIONAL (D), with 16 BINS (K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DiscretisedBimodalData(k=16)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "print(batch.shape)\n",
    "# plt.hist(batch.numpy(), bins=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = bfn_model.sample_generation_for_discretised_data(n_steps=20, bs=1024).to(torch.float32)\n",
    "plt.hist(samples.detach().numpy(), bins=16)\n",
    "plt.hist(batch.numpy(), bins=16, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ls ../bfn_github/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bfn_github.bfn import BayesianFlowNetwork\n",
    "import torch\n",
    "# network should learn:\n",
    "# when x0 = 0, x1 = 1\n",
    "# when x0 = 1, x1 = 0\n",
    "def get_datapoint(batch=128, device='cpu'):\n",
    "    x0 = torch.randint(0, 2, size=(batch,), dtype=torch.bool, device=device)\n",
    "    x1 = ~x0\n",
    "\n",
    "    X = torch.stack([x0, x1], dim=0)\n",
    "    return X.long().transpose(0, 1)\n",
    "\n",
    "X = get_datapoint()  # (B, D=2) with K=2 classes \n",
    "print(X.shape)\n",
    "# plt.title(\"Dataset\")\n",
    "# plt.scatter(X[:, 0], X[:, 1]);\n",
    "# plt.grid()\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "bfn = BayesianFlowNetwork()\n",
    "\n",
    "optim = AdamW(bfn.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "n = 1000\n",
    "losses = []\n",
    "for i in tqdm(range(n)):\n",
    "    optim.zero_grad()\n",
    "    X = get_datapoint(device='cpu')\n",
    "    loss = bfn.process(X)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "bfn.sample(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.one_hot(torch.tensor([2, 1]).unsqueeze(0), num_classes=3)\n",
    "def vectorised_discretised_cdf(mu, sigma, bounds):\n",
    "    # input is mu, sigma -> B x D, and bounds -> K\n",
    "    lower_mask = bounds < -1\n",
    "    upper_mask = bounds > 1\n",
    "\n",
    "    # output is B x D x K\n",
    "    result = torch.zeros(mu.shape[0], mu.shape[1], bounds.shape[0])\n",
    "    result = 0.5 * (1 + torch.erf((bounds - mu.unsqueeze(-1)) / (sigma.unsqueeze(-1) * torch.sqrt(torch.tensor(2.0)))))\n",
    "    # clip result depending on boundsa\n",
    "    result[:, :, lower_mask] = 0\n",
    "    result[:, :, upper_mask] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "mu = torch.randn(2, 2)\n",
    "sigma = torch.ones(2, 2)\n",
    "bounds = torch.linspace(-1.1, 1.1, 4)\n",
    "\n",
    "cdf = vectorised_discretised_cdf(mu, sigma, bounds)\n",
    "\n",
    "print(cdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
